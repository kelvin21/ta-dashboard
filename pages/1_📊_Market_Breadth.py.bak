import streamlit as st
import os
import sys

# Check if page should be visible
if os.getenv("SHOW_MARKET_BREADTH_PAGE", "true").lower() == "false":
    st.error("This page is not available in the current deployment.")
    st.stop()

import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import asyncio

# Try to import TA-Lib
try:
    import talib
    HAS_TALIB = True
except ImportError:
    HAS_TALIB = False

# Add parent directory to path to import shared modules
SCRIPT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if SCRIPT_DIR not in sys.path:
    sys.path.insert(0, SCRIPT_DIR)

# Import new utility modules (optional - fallback to existing functions if not available)
try:
    from utils.indicators import calculate_all_indicators, categorize_rsi, check_price_above_ema, HAS_TALIB as UTILS_HAS_TALIB
    from utils.macd_stage import detect_macd_stage as utils_detect_macd_stage, categorize_macd_stage
    from utils.db_async import get_sync_db_adapter, HAS_MOTOR
    USE_UTILS = True
    # Update HAS_TALIB if utils has it
    if not HAS_TALIB and UTILS_HAS_TALIB:
        HAS_TALIB = UTILS_HAS_TALIB
except ImportError:
    USE_UTILS = False

# Try to import database adapter
try:
    from db_adapter import get_db_adapter
    db = get_db_adapter()
    HAS_DB_ADAPTER = True
except ImportError:
    db = None
    HAS_DB_ADAPTER = False

# Import from main dashboard with better error handling
try:
    from ta_dashboard import (
        DB_PATH, DEFAULT_LOCAL_DB, HAS_BDB,
        load_price_range, get_all_tickers, macd_hist, detect_stage
    )
except ImportError as e:
    st.error(f"""
    ‚ùå Could not import required functions from ta_dashboard.py
    
    **Error Details:**
    {str(e)}
    
    **Troubleshooting:**
    1. Ensure ta_dashboard.py is in the parent directory (c:\\Users\\hadao\\OneDrive\\Documents\\Programming\\macd-reversal\\)
    2. Check that ta_dashboard.py is not corrupted
    3. Try restarting Streamlit: `streamlit run ta_dashboard.py`
    
    **Current Directory Structure:**
    - Script directory: {SCRIPT_DIR}
    - Pages directory: {os.path.dirname(os.path.abspath(__file__))}
    - Files in script directory: {os.listdir(SCRIPT_DIR)[:10]}
    """)
    st.stop()

st.set_page_config(page_title="Market Breadth", layout="wide", page_icon="üìä")
st.title("üìä Market Breadth Analysis")

# Show TA-Lib status
if HAS_TALIB:
    st.info("‚úÖ Using TA-Lib for technical indicators (most accurate)")
else:
    st.warning("‚ö†Ô∏è TA-Lib not installed. Using manual calculations. For best accuracy, install: `pip install TA-Lib`")

# Sidebar controls
st.sidebar.header("Analysis Settings")
lookback = st.sidebar.slider("MACD Lookback (bars)", 5, 60, 20)
days_back = st.sidebar.number_input("Days of history", 200, 730, 365)
debug = st.sidebar.checkbox("Show debug info", value=False)

st.sidebar.info("üí° **Tip:** Market Breadth requires 200+ bars (‚âà10 months) for MA200 calculation.")

# Historical data settings
st.sidebar.markdown("---")
st.sidebar.markdown("### üìä Historical Charts")
show_historical = st.sidebar.checkbox("Show historical breadth charts", value=True)
historical_days = st.sidebar.slider("Historical period (days)", 10, 730, 548) if show_historical else 365

# New: Add date picker for viewing historical snapshot
st.sidebar.markdown("---")
st.sidebar.markdown("### üìÖ View Historical Snapshot")
view_historical_snapshot = st.sidebar.checkbox("View specific date snapshot", value=False)
selected_date = None

if view_historical_snapshot:
    selected_date = st.sidebar.date_input(
        "Select date",
        value=datetime.now().date(),
        help="Choose a date to view market breadth for that day"
    )

# Recalculate button
if st.sidebar.button("üîÑ Recalculate Historical Data", help="Recalculate and save historical market breadth data"):
    st.session_state['recalculate_breadth'] = True

# Load all tickers
all_tickers = get_all_tickers(debug=False)

if not all_tickers:
    st.warning("No tickers found in database")
    st.stop()

st.info(f"Analyzing {len(all_tickers)} tickers...")

# Calculate breadth metrics
end_date = datetime.now().date()
start_date = end_date - timedelta(days=days_back)

@st.cache_data(ttl=300)
def calculate_breadth_metrics(tickers, start_date, end_date, lookback, debug=False):
    """Calculate market breadth metrics for all tickers using TA-Lib if available."""
    results = []
    errors = []
    skipped_reasons = {"empty": 0, "too_short": 0, "error": 0}
    
    for ticker in tickers:
        try:
            df = load_price_range(ticker, start_date, end_date)
            
            if df.empty:
                skipped_reasons["empty"] += 1
                if debug:
                    errors.append(f"{ticker}: Empty dataframe")
                continue
            
            # Sort by date in ascending order for proper calculations
            df = df.sort_values('date', ascending=True).reset_index(drop=True)
            
            if len(df) < 200:  # Need at least 200 bars for MA200
                skipped_reasons["too_short"] += 1
                if debug:
                    errors.append(f"{ticker}: Only {len(df)} bars (need 200+)")
                continue
            
            # Get latest data - MUST BE LAST ROW after sorting ASCENDING
            latest = df.iloc[-1]
            close = float(latest['close'])
            latest_date = latest['date']
            
            if debug:
                errors.append(f"{ticker}: Latest date = {latest_date}, close = {close}")
            
            # Calculate moving averages using TA-Lib if available
            if HAS_TALIB:
                try:
                    close_array = df['close'].values.astype(np.float64)
                    ma20 = talib.SMA(close_array, timeperiod=20)
                    ma50 = talib.SMA(close_array, timeperiod=50)
                    ma200 = talib.SMA(close_array, timeperiod=200)
                    rsi = talib.RSI(close_array, timeperiod=14)
                except Exception as e:
                    if debug:
                        errors.append(f"{ticker}: TA-Lib error - {str(e)}")
                    # Fallback to manual calculation
                    ma20 = df['close'].rolling(20).mean().values
                    ma50 = df['close'].rolling(50).mean().values
                    ma200 = df['close'].rolling(200).mean().values
                    rsi = calculate_rsi_manual(df)
            else:
                # Manual fallback if TA-Lib not available
                ma20 = df['close'].rolling(20).mean().values
                ma50 = df['close'].rolling(50).mean().values
                ma200 = df['close'].rolling(200).mean().values
                rsi = calculate_rsi_manual(df)
            
            # Calculate MACD using TA-Lib if available
            if HAS_TALIB:
                try:
                    close_array = df['close'].values.astype(np.float64)
                    macd_line, macd_signal, macd_hist = talib.MACD(close_array, fastperiod=12, slowperiod=26, signalperiod=9)
                    hist = pd.Series(macd_hist)
                except Exception as e:
                    if debug:
                        errors.append(f"{ticker}: MACD TA-Lib error - {str(e)}")
                    _, _, hist = macd_hist(df['close'].astype(float))
            else:
                _, _, hist = macd_hist(df['close'].astype(float))
            
            stage = detect_stage(hist, lookback=lookback)
            
            # Get latest values - MUST USE -1 INDEX FOR LATEST
            latest_ma20 = float(ma20[-1]) if not np.isnan(ma20[-1]) else None
            latest_ma50 = float(ma50[-1]) if not np.isnan(ma50[-1]) else None
            latest_ma200 = float(ma200[-1]) if not np.isnan(ma200[-1]) else None
            
            # FIXED: Better RSI handling
            if isinstance(rsi, np.ndarray):
                latest_rsi = float(rsi[-1]) if len(rsi) > 0 and not np.isnan(rsi[-1]) else None
            else:
                latest_rsi = None
                if debug:
                    errors.append(f"{ticker}: RSI calculation failed - invalid type {type(rsi)}")
            
            # FIXED: Ensure all boolean comparisons work
            above_ma20 = close > latest_ma20 if latest_ma20 is not None else None
            above_ma50 = close > latest_ma50 if latest_ma50 is not None else None  
            above_ma200 = close > latest_ma200 if latest_ma200 is not None else None
            
            results.append({
                'ticker': ticker,
                'date': latest_date,  # Store actual latest date
                'close': close,
                'above_ma20': above_ma20,
                'above_ma50': above_ma50,
                'above_ma200': above_ma200,
                'rsi': latest_rsi,
                'macd_stage': stage
            })
        except Exception as e:
            skipped_reasons["error"] += 1
            if debug:
                errors.append(f"{ticker}: {str(e)[:100]}")
            continue
    
    return pd.DataFrame(results), errors, skipped_reasons

def calculate_rsi_manual(df, period=14):
    """Manual RSI calculation fallback using Wilder's smoothing - IMPROVED."""
    try:
        if len(df) < period + 1:
            return np.full(len(df), np.nan)
            
        # Ensure we have numeric data
        close_prices = pd.to_numeric(df['close'], errors='coerce')
        if close_prices.isna().all():
            return np.full(len(df), np.nan)
        
        delta = close_prices.diff()
        gain = delta.where(delta > 0, 0)
        loss = -delta.where(delta < 0, 0)
        
        rsi = np.full(len(df), np.nan)
        
        if len(df) >= period + 1:
            # Initial average
            avg_gain = gain.iloc[1:period+1].mean()
            avg_loss = loss.iloc[1:period+1].mean()
            
            # Calculate RSI for each point after the initial period
            for i in range(period, len(df)):
                if i == period:
                    rs = avg_gain / avg_loss if avg_loss > 0 else 0
                    rsi[i] = 100 - (100 / (1 + rs)) if rs > 0 else (100 if avg_gain > 0 else 50)
                else:
                    # Wilder's smoothing
                    avg_gain = (avg_gain * (period - 1) + gain.iloc[i]) / period
                    avg_loss = (avg_loss * (period - 1) + loss.iloc[i]) / period
                    rs = avg_gain / avg_loss if avg_loss > 0 else 0
                    rsi[i] = 100 - (100 / (1 + rs)) if rs > 0 else (100 if avg_gain > 0 else 50)
        
        return rsi
    except Exception as e:
        # Return NaN array if calculation fails
        return np.full(len(df), np.nan)

# Database functions for historical breadth data
def create_breadth_history_table(db_path=DB_PATH):
    """Create table/collection for storing historical market breadth data."""
    if HAS_DB_ADAPTER:
        # MongoDB/db_adapter handles schema implicitly
        return True
    else:
        # SQLite fallback
        conn = sqlite3.connect(db_path)
        cur = conn.cursor()
        cur.execute("""
            CREATE TABLE IF NOT EXISTS market_breadth_history (
                date TEXT PRIMARY KEY,
                ma20_above INTEGER,
                ma20_pct REAL,
                ma50_above INTEGER,
                ma50_pct REAL,
                ma200_above INTEGER,
                ma200_pct REAL,
                rsi_above_50 INTEGER,
                rsi_below_50 INTEGER,
                rsi_oversold INTEGER,
                rsi_overbought INTEGER,
                macd_troughing INTEGER,
                macd_confirmed_trough INTEGER,
                macd_rising INTEGER,
                macd_peaking INTEGER,
                macd_confirmed_peak INTEGER,
                macd_falling INTEGER,
                total_tickers INTEGER,
                calculated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        conn.commit()
        conn.close()
        return True

def load_breadth_history(days=548, db_path=DB_PATH):
    """Load historical breadth data from database (MongoDB or SQLite)."""
    if HAS_DB_ADAPTER and db is not None:
        try:
            if hasattr(db, 'load_breadth_history') and callable(getattr(db, 'load_breadth_history')):
                end_date = datetime.now()
                start_date = end_date - timedelta(days=days)
                
                df = db.load_breadth_history(start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d"))
                
                if not df.empty:
                    df['date'] = pd.to_datetime(df['date'])
                    
                    # Ensure numeric types
                    numeric_cols = [
                        'ma20_above', 'ma20_pct', 'ma50_above', 'ma50_pct', 
                        'ma200_above', 'ma200_pct', 'rsi_above_50', 'rsi_below_50',
                        'rsi_oversold', 'rsi_overbought', 'macd_troughing',
                        'macd_confirmed_trough', 'macd_rising', 'macd_peaking',
                        'macd_confirmed_peak', 'macd_falling', 'total_tickers'
                    ]
                    
                    for col in numeric_cols:
                        if col in df.columns:
                            if col.endswith('_pct'):
                                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0).astype(float)
                            else:
                                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)
                    
                    # Sort by date ASCENDING (earliest first) for consistent time series
                    df = df.sort_values('date', ascending=True).reset_index(drop=True)
                    return df
        except (AttributeError, Exception):
            pass
    
    # SQLite fallback
    conn = sqlite3.connect(db_path)
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)
    
    query = """
        SELECT * FROM market_breadth_history 
        WHERE date >= ? 
        ORDER BY date ASC
    """
    
    df = pd.read_sql_query(query, conn, params=(start_date.strftime("%Y-%m-%d"),))
    conn.close()
    
    if not df.empty:
        df['date'] = pd.to_datetime(df['date'])
        
        numeric_cols = [
            'ma20_above', 'ma20_pct', 'ma50_above', 'ma50_pct', 
            'ma200_above', 'ma200_pct', 'rsi_above_50', 'rsi_below_50',
            'rsi_oversold', 'rsi_overbought', 'macd_troughing',
            'macd_confirmed_trough', 'macd_rising', 'macd_peaking',
            'macd_confirmed_peak', 'macd_falling', 'total_tickers'
        ]
        
        for col in numeric_cols:
            if col in df.columns:
                if col.endswith('_pct'):
                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0).astype(float)
                else:
                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)
    
    return df

def save_breadth_snapshot(date_str, breadth_data, db_path=DB_PATH):
    """Save a daily breadth snapshot to database (MongoDB or SQLite)."""
    if HAS_DB_ADAPTER and db is not None:
        try:
            if hasattr(db, 'save_breadth_snapshot') and callable(getattr(db, 'save_breadth_snapshot')):
                doc = {
                    'date': date_str,
                    'ma20_above': int(breadth_data['ma20_above']),
                    'ma20_pct': float(breadth_data['ma20_pct']),
                    'ma50_above': int(breadth_data['ma50_above']),
                    'ma50_pct': float(breadth_data['ma50_pct']),
                    'ma200_above': int(breadth_data['ma200_above']),
                    'ma200_pct': float(breadth_data['ma200_pct']),
                    'rsi_above_50': int(breadth_data['rsi_above_50']),
                    'rsi_below_50': int(breadth_data['rsi_below_50']),
                    'rsi_oversold': int(breadth_data['rsi_oversold']),
                    'rsi_overbought': int(breadth_data['rsi_overbought']),
                    'macd_troughing': int(breadth_data['macd_troughing']),
                    'macd_confirmed_trough': int(breadth_data['macd_confirmed_trough']),
                    'macd_rising': int(breadth_data['macd_rising']),
                    'macd_peaking': int(breadth_data['macd_peaking']),
                    'macd_confirmed_peak': int(breadth_data['macd_confirmed_peak']),
                    'macd_falling': int(breadth_data['macd_falling']),
                    'total_tickers': int(breadth_data['total_tickers'])
                }
                return db.save_breadth_snapshot(date_str, doc)
        except (AttributeError, Exception):
            pass
    
    # SQLite fallback
    conn = sqlite3.connect(db_path)
    cur = conn.cursor()
    
    cur.execute("""
        INSERT OR REPLACE INTO market_breadth_history 
        (date, ma20_above, ma20_pct, ma50_above, ma50_pct, ma200_above, ma200_pct,
         rsi_above_50, rsi_below_50, rsi_oversold, rsi_overbought,
         macd_troughing, macd_confirmed_trough, macd_rising, macd_peaking, 
         macd_confirmed_peak, macd_falling, total_tickers)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """, (
        date_str,
        breadth_data['ma20_above'], breadth_data['ma20_pct'],
        breadth_data['ma50_above'], breadth_data['ma50_pct'],
        breadth_data['ma200_above'], breadth_data['ma200_pct'],
        breadth_data['rsi_above_50'], breadth_data['rsi_below_50'],
        breadth_data['rsi_oversold'], breadth_data['rsi_overbought'],
        breadth_data['macd_troughing'], breadth_data['macd_confirmed_trough'],
        breadth_data['macd_rising'], breadth_data['macd_peaking'],
        breadth_data['macd_confirmed_peak'], breadth_data['macd_falling'],
        breadth_data['total_tickers']
    ))
    
    conn.commit()
    conn.close()
    return True

# Additional helper functions for historical breadth calculation
def calculate_daily_breadth(date_str, all_tickers, lookback=20, db_path=DB_PATH):
    """Calculate breadth metrics for a specific date (synchronous version) - IMPROVED."""
    try:
        # Set date range: need 250+ days before target date for MA200 calculation
        target_date = datetime.strptime(date_str, "%Y-%m-%d")
        calc_start = target_date - timedelta(days=300)
        calc_end = target_date
        
        # Calculate breadth metrics for the date range
        breadth_df, _, _ = calculate_breadth_metrics(
            all_tickers,
            calc_start.strftime("%Y-%m-%d"),
            calc_end.strftime("%Y-%m-%d"),
            lookback,
            False
        )
        
        if breadth_df.empty:
            return None
        
        # Filter to only tickers where latest_date == target_date
        breadth_df = breadth_df[breadth_df['date'] == target_date.date()].reset_index(drop=True)
        
        if breadth_df.empty:
            return None
        
        # Calculate all metrics with improved error handling
        total = len(breadth_df)
        
        # FIXED: Handle None values in boolean columns
        ma20_above = int(breadth_df['above_ma20'].fillna(False).sum())
        ma50_above = int(breadth_df['above_ma50'].fillna(False).sum())
        ma200_above = int(breadth_df['above_ma200'].fillna(False).sum())
        
        # FIXED: Better RSI handling
        rsi_df = breadth_df[breadth_df['rsi'].notna() & (breadth_df['rsi'] >= 0) & (breadth_df['rsi'] <= 100)]
        total_rsi = len(rsi_df) if len(rsi_df) > 0 else 1
        rsi_above_50 = int((rsi_df['rsi'] > 50).sum()) if len(rsi_df) > 0 else 0
        rsi_below_50 = int((rsi_df['rsi'] <= 50).sum()) if len(rsi_df) > 0 else 0
        rsi_oversold = int((rsi_df['rsi'] < 30).sum()) if len(rsi_df) > 0 else 0
        rsi_overbought = int((rsi_df['rsi'] > 70).sum()) if len(rsi_df) > 0 else 0
        
        stage_counts = breadth_df['macd_stage'].value_counts()
        
        return {
            'ma20_above': ma20_above,
            'ma20_pct': (ma20_above / total * 100) if total > 0 else 0,
            'ma50_above': ma50_above,
            'ma50_pct': (ma50_above / total * 100) if total > 0 else 0,
            'ma200_above': ma200_above,
            'ma200_pct': (ma200_above / total * 100) if total > 0 else 0,
            'rsi_above_50': rsi_above_50,
            'rsi_below_50': rsi_below_50,
            'rsi_oversold': rsi_oversold,
            'rsi_overbought': rsi_overbought,
            'macd_troughing': int(stage_counts.get("1. Troughing", 0)),
            'macd_confirmed_trough': int(stage_counts.get("2. Confirmed Trough", 0)),
            'macd_rising': int(stage_counts.get("3. Rising above Zero", 0)),
            'macd_peaking': int(stage_counts.get("4. Peaking", 0)),
            'macd_confirmed_peak': int(stage_counts.get("5. Confirmed Peak", 0)),
            'macd_falling': int(stage_counts.get("6. Falling below Zero", 0)),
            'total_tickers': total
        }
    except Exception as e:
        return None

async def calculate_daily_breadth_async(date_str, all_tickers, lookback=20):
    """Calculate breadth metrics for a specific date asynchronously - IMPROVED ERROR HANDLING."""
    try:
        import asyncio
        
        # Set date range: need 250+ days before target date for MA200 calculation
        target_date = datetime.strptime(date_str, "%Y-%m-%d")
        calc_start = target_date - timedelta(days=300)
        calc_end = target_date
        
        # Run synchronous calculation in thread pool with timeout
        loop = asyncio.get_event_loop()
        
        try:
            breadth_df, errors, skipped_reasons = await asyncio.wait_for(
                loop.run_in_executor(
                    None,
                    calculate_breadth_metrics,
                    all_tickers,
                    calc_start.strftime("%Y-%m-%d"),
                    calc_end.strftime("%Y-%m-%d"),
                    lookback,
                    True  # Enable debug for detailed error tracking
                ),
                timeout=60.0  # 60 second timeout
            )
            
            # IMPROVED: Check for calculation issues and report them
            total_tickers = len(all_tickers)
            successful_tickers = len(breadth_df)
            failed_tickers = total_tickers - successful_tickers
            
            # If too many tickers failed, report the specific errors
            if failed_tickers > (total_tickers * 0.5):  # More than 50% failed
                error_summary = []
                error_summary.append(f"High failure rate: {failed_tickers}/{total_tickers} tickers failed")
                error_summary.append(f"Skipped reasons: {skipped_reasons}")
                
                # Sample first 10 errors for diagnosis
                if errors:
                    error_summary.append("Sample errors:")
                    for i, error in enumerate(errors[:10]):
                        error_summary.append(f"  {i+1}. {error}")
                    if len(errors) > 10:
                        error_summary.append(f"  ... and {len(errors) - 10} more errors")
                
                return Exception(f"High failure rate for {date_str}: {'; '.join(error_summary)}")
            
            # If some errors but not catastrophic, log them but continue
            if errors and len(errors) > 10:
                error_sample = errors[:5]  # First 5 errors
                return Exception(f"Partial calculation errors for {date_str}: {'; '.join(error_sample)} (and {len(errors) - 5} more)")
            
        except asyncio.TimeoutError:
            return Exception(f"Timeout calculating breadth for {date_str}")
        except Exception as calc_error:
            return Exception(f"Calculation error for {date_str}: {str(calc_error)}")
        
        if breadth_df.empty:
            return Exception(f"Empty breadth DataFrame for {date_str} - no tickers passed filtering")
        
        # Filter to only tickers where latest_date == target_date
        original_count = len(breadth_df)
        breadth_df = breadth_df[breadth_df['date'] == target_date.date()].reset_index(drop=True)
        
        if breadth_df.empty:
            return Exception(f"No data for target date {date_str} (filtered DataFrame is empty). Original count: {original_count}, dates available: {breadth_df['date'].unique() if original_count > 0 else 'none'}")
        
        # Calculate all metrics with better error handling
        total = len(breadth_df)
        
        if total == 0:
            return Exception(f"Zero tickers found for {date_str}")
        
        # IMPROVED: Handle None values in boolean columns with detailed error tracking
        try:
            # Check for None values before aggregation
            ma20_none_count = breadth_df['above_ma20'].isna().sum()
            ma50_none_count = breadth_df['above_ma50'].isna().sum()
            ma200_none_count = breadth_df['above_ma200'].isna().sum()
            
            ma20_above = int(breadth_df['above_ma20'].fillna(False).sum())
            ma50_above = int(breadth_df['above_ma50'].fillna(False).sum())
            ma200_above = int(breadth_df['above_ma200'].fillna(False).sum())
            
            # Report if many None values
            if ma20_none_count > (total * 0.3) or ma50_none_count > (total * 0.3) or ma200_none_count > (total * 0.3):
                return Exception(f"High None count for {date_str}: MA20 None: {ma20_none_count}/{total}, MA50 None: {ma50_none_count}/{total}, MA200 None: {ma200_none_count}/{total}")
                
        except Exception as ma_error:
            return Exception(f"MA calculation error for {date_str}: {str(ma_error)}")
        
        # IMPROVED: Better RSI handling with detailed diagnostics
        try:
            # Check RSI data quality
            rsi_total_count = breadth_df['rsi'].notna().sum()
            rsi_valid_range = breadth_df[(breadth_df['rsi'] >= 0) & (breadth_df['rsi'] <= 100)].shape[0]
            rsi_invalid_count = breadth_df[(breadth_df['rsi'] < 0) | (breadth_df['rsi'] > 100)].notna().sum()
            
            rsi_df = breadth_df[breadth_df['rsi'].notna() & (breadth_df['rsi'] >= 0) & (breadth_df['rsi'] <= 100)]
            total_rsi = len(rsi_df) if len(rsi_df) > 0 else 1
            rsi_above_50 = int((rsi_df['rsi'] > 50).sum()) if len(rsi_df) > 0 else 0
            rsi_below_50 = int((rsi_df['rsi'] <= 50).sum()) if len(rsi_df) > 0 else 0
            rsi_oversold = int((rsi_df['rsi'] < 30).sum()) if len(rsi_df) > 0 else 0
            rsi_overbought = int((rsi_df['rsi'] > 70).sum()) if len(rsi_df) > 0 else 0
            
            # Report RSI issues if significant
            if rsi_total_count < (total * 0.5):  # Less than 50% have valid RSI
                return Exception(f"RSI data quality issue for {date_str}: Valid RSI: {rsi_total_count}/{total}, Valid range: {rsi_valid_range}, Invalid: {rsi_invalid_count}")
            
        except Exception as rsi_error:
            return Exception(f"RSI calculation error for {date_str}: {str(rsi_error)}")
        
        try:
            stage_counts = breadth_df['macd_stage'].value_counts()
            
            # Check for MACD stage issues
            stage_none_count = breadth_df['macd_stage'].isna().sum();
            if stage_none_count > (total * 0.3):  # More than 30% have no stage
                return Exception(f"MACD stage issue for {date_str}: {stage_none_count}/{total} tickers have no MACD stage")
                
        except Exception as macd_error:
            return Exception(f"MACD stage calculation error for {date_str}: {str(macd_error)}")
        
        # IMPROVED: Add data quality summary to return value
        result = {
            'ma20_above': ma20_above,
            'ma20_pct': (ma20_above / total * 100) if total > 0 else 0,
            'ma50_above': ma50_above,
            'ma50_pct': (ma50_above / total * 100) if total > 0 else 0,
            'ma200_above': ma200_above,
            'ma200_pct': (ma200_above / total * 100) if total > 0 else 0,
            'rsi_above_50': rsi_above_50,
            'rsi_below_50': rsi_below_50,
            'rsi_oversold': rsi_oversold,
            'rsi_overbought': rsi_overbought,
            'macd_troughing': int(stage_counts.get("1. Troughing", 0)),
            'macd_confirmed_trough': int(stage_counts.get("2. Confirmed Trough", 0)),
            'macd_rising': int(stage_counts.get("3. Rising above Zero", 0)),
            'macd_peaking': int(stage_counts.get("4. Peaking", 0)),
            'macd_confirmed_peak': int(stage_counts.get("5. Confirmed Peak", 0)),
            'macd_falling': int(stage_counts.get("6. Falling below Zero", 0)),
            'total_tickers': total,
            # IMPROVED: Add quality metrics for debugging
            '_debug_info': {
                'original_ticker_count': len(all_tickers),
                'successful_tickers': successful_tickers,
                'failed_tickers': failed_tickers,
                'error_count': len(errors),
                'skipped_reasons': skipped_reasons,
                'rsi_valid_count': rsi_total_count,
                'rsi_invalid_count': rsi_invalid_count,
                'ma_none_counts': {
                    'ma20': ma20_none_count,
                    'ma50': ma50_none_count, 
                    'ma200': ma200_none_count
                },
                'stage_none_count': stage_none_count
            }
        }
        
        return result
        
    except Exception as e:
        # Return detailed exception object to be handled by caller
        return Exception(f"Async calculation failed for {date_str}: {str(e)}")

# --- Main dashboard code ---

# Calculate breadth metrics for current snapshot
with st.spinner("Calculating market breadth metrics..."):
    breadth_df, errors, skipped_reasons = calculate_breadth_metrics(
        all_tickers, 
        start_date.strftime("%Y-%m-%d"), 
        end_date.strftime("%Y-%m-%d"), 
        lookback=lookback, 
        debug=debug
    )

if breadth_df.empty:
    st.error("‚ö†Ô∏è Not enough data to calculate breadth metrics")
    
    # Show debug information
    st.markdown("### üîç Debug Information")
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total Tickers", len(all_tickers))
    with col2:
        st.metric("Date Range", f"{days_back} days")
    with col3:
        st.metric("Required Bars", "200+")
    with col4:
        st.metric("Start Date", start_date.strftime("%Y-%m-%d"))
    
    st.markdown("#### Skipped Tickers Breakdown")
    skip_df = pd.DataFrame([
        {"Reason": "Empty data", "Count": skipped_reasons["empty"], "Impact": f"{skipped_reasons['empty']/len(all_tickers)*100:.1f}%"},
        {"Reason": "Too short (<200 bars)", "Count": skipped_reasons["too_short"], "Impact": f"{skipped_reasons['too_short']/len(all_tickers)*100:.1f}%"},
        {"Reason": "Errors", "Count": skipped_reasons["error"], "Impact": f"{skipped_reasons['error']/len(all_tickers)*100:.1f}%"},
    ])
    st.dataframe(skip_df, width='stretch')
    
    if debug and errors:
        st.markdown("#### Detailed Errors")
        with st.expander(f"Show {len(errors)} error messages"):
            for err in errors[:50]:
                st.text(err)
            if len(errors) > 50:
                st.caption(f"... and {len(errors) - 50} more errors")
    
    st.stop()

st.success(f"Successfully analyzed {len(breadth_df)} tickers")

# FIXED: Add RSI debugging info for current snapshot
if debug:
    st.markdown("### üîç Current RSI Debug Information")
    rsi_debug_df = breadth_df[['ticker', 'rsi']].copy()
    rsi_debug_df['rsi_valid'] = rsi_debug_df['rsi'].notna()
    rsi_debug_df['rsi_in_range'] = (rsi_debug_df['rsi'] >= 0) & (rsi_debug_df['rsi'] <= 100)
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        valid_rsi = rsi_debug_df['rsi_valid'].sum()
        st.metric("Tickers with RSI", f"{valid_rsi}/{len(rsi_debug_df)}")
    with col2:
        in_range_rsi = rsi_debug_df['rsi_in_range'].sum()
        st.metric("Valid RSI Range", f"{in_range_rsi}/{valid_rsi}")
    with col3:
        rsi_above_50_debug = (rsi_debug_df['rsi'] > 50).sum()
        st.metric("RSI > 50", f"{rsi_above_50_debug}/{valid_rsi}")
    with col4:
        avg_rsi = rsi_debug_df['rsi'].mean()
        st.metric("Avg RSI", f"{avg_rsi:.1f}" if not pd.isna(avg_rsi) else "N/A")

# Display current date - use actual latest date from data
if not breadth_df.empty and 'date' in breadth_df.columns:
    # Get the most recent date from all tickers
    latest_data_date = breadth_df['date'].max()
    current_date_str = latest_data_date.strftime("%Y-%m-%d")
    st.markdown(f"### Market Breadth Snapshot ‚Äî {current_date_str}")
    
    if debug:
        st.caption(f"**Debug:** Latest dates in data:")
        date_counts = breadth_df['date'].value_counts().sort_index(ascending=False)
        for date, count in date_counts.head(5).items():
            st.caption(f"  {date.strftime('%Y-%m-%d')}: {count} tickers")
else:
    current_date_str = datetime.now().strftime("%Y-%m-%d")
    st.markdown(f"### Market Breadth Snapshot ‚Äî {current_date_str}")

# --- Load historical snapshot data if date picker is used ---
# MODIFIED: Auto-load latest date OR use selected date
if view_historical_snapshot:
    # If no specific date selected, load the latest available date
    if selected_date is None:
        selected_date = datetime.now().date()
    
    selected_date_str = selected_date.strftime("%Y-%m-%d")
    
    # Load from database
    historical_snapshot_data = load_breadth_history(days=1000, db_path=DB_PATH)  # Load more data to find latest
    
    if historical_snapshot_data.empty:
        st.warning(f"üì≠ No historical data found. Click 'üîÑ Recalculate Historical Data' to generate it.")
    else:
        # Find the latest available date in the database
        latest_available_date = historical_snapshot_data['date'].max().date()
        
        # Filter to selected date OR latest date if not found
        filtered_data = historical_snapshot_data[historical_snapshot_data['date'] == selected_date_str]
        
        if filtered_data.empty:
            # If selected date has no data, use latest available
            selected_date_str = latest_available_date.strftime("%Y-%m-%d")
            filtered_data = historical_snapshot_data[historical_snapshot_data['date'] == selected_date_str]
            st.info(f"‚ÑπÔ∏è No data for selected date. Showing latest available: **{selected_date_str}**")
        
        if filtered_data.empty:
            st.warning(f"üì≠ No historical data found for {selected_date_str}.")
        else:
            st.success(f"‚úÖ Loaded historical data for {selected_date_str}")
            
            # Display metrics like current snapshot
            latest = filtered_data.iloc[-1]
            st.markdown(f"### Market Breadth Snapshot ‚Äî {selected_date_str}")
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Total Tickers", latest['total_tickers'])
            with col2:
                st.metric("Above MA20", f"{latest['ma20_above']}/{latest['total_tickers']}", f"{latest['ma20_pct']:.1f}%")
            with col3:
                st.metric("Above MA50", f"{latest['ma50_above']}/{latest['total_tickers']}", f"{latest['ma50_pct']:.1f}%")
            with col4:
                st.metric("Above MA200", f"{latest['ma200_above']}/{latest['total_tickers']}", f"{latest['ma200_pct']:.1f}%")
            
            # --- Debug info for historical snapshot ---
            debug_data = []  # Initialize outside the if block
            
            if debug:
                st.markdown("### üîç Debug: Historical Snapshot Details")
                st.write(filtered_data)
                
                # Show detailed calculations for current tickers (not historical individual ticker data)
                st.info("**Note:** Debug shows current ticker calculations. Historical data only stores aggregated metrics.")
                
                with st.spinner("Loading current ticker data for debug comparison..."):
                    # Use current breadth_df for debug, not historical individual ticker data
                    for ticker in breadth_df['ticker'].tolist()[:50]:  # Use current breadth data
                        try:
                            df = load_price_range(ticker, start_date, end_date)
                            if df.empty or len(df) < 14:
                                continue
                            
                            # Sort ascending for proper calculations
                            df = df.sort_values('date', ascending=True).reset_index(drop=True)
                            
                            # Get latest bar - MUST BE LAST ROW
                            latest_ticker_data = df.iloc[-1]
                            latest_date = latest_ticker_data['date']
                            close = float(latest_ticker_data['close'])
                            
                            # Calculate indicators using TA-Lib if available
                            close_array = df['close'].values.astype(np.float64)
                            
                            if HAS_TALIB:
                                try:
                                    ma20_arr = talib.SMA(close_array, timeperiod=20)
                                    ma50_arr = talib.SMA(close_array, timeperiod=50)
                                    ma200_arr = talib.SMA(close_array, timeperiod=200)
                                    rsi_arr = talib.RSI(close_array, timeperiod=14)
                                    macd_line, macd_signal, macd_hist_vals = talib.MACD(close_array, fastperiod=12, slowperiod=26, signalperiod=9)
                                    
                                    debug_data.append({
                                        'Ticker': ticker,
                                        'Date': latest_date.strftime("%Y-%m-%d"),
                                        'Close': f"{close:.2f}",
                                        'MA20': f"{ma20_arr[-1]:.2f}" if not np.isnan(ma20_arr[-1]) else "N/A",
                                        'MA50': f"{ma50_arr[-1]:.2f}" if not np.isnan(ma50_arr[-1]) else "N/A",
                                        'MA200': f"{ma200_arr[-1]:.2f}" if not np.isnan(ma200_arr[-1]) else "N/A",
                                        'RSI': f"{rsi_arr[-1]:.2f}" if not np.isnan(rsi_arr[-1]) else "N/A",
                                        'MACD_Hist': f"{macd_hist_vals[-1]:.6f}" if not np.isnan(macd_hist_vals[-1]) else "N/A",
                                        'MACD_Stage': breadth_df[breadth_df['ticker'] == ticker]['macd_stage'].iloc[0] if not breadth_df[breadth_df['ticker'] == ticker].empty else "N/A",
                                        'Bars': len(df)
                                    })
                                except Exception as e:
                                    st.caption(f"‚ö†Ô∏è {ticker}: TA-Lib error - {str(e)[:50]}")
                            else:
                                # Manual calculation
                                ma20_val = df['close'].rolling(20).mean().iloc[-1]
                                ma50_val = df['close'].rolling(50).mean().iloc[-1]
                                ma200_val = df['close'].rolling(200).mean().iloc[-1]
                                
                                delta = df['close'].diff()
                                gain = delta.where(delta > 0, 0)
                                loss = -delta.where(delta < 0, 0)
                                avg_gain = gain.rolling(14).mean().iloc[-1]
                                avg_loss = loss.rolling(14).mean().iloc[-1]
                                rs = avg_gain / avg_loss if avg_loss > 0 else 0
                                rsi_val = 100 - (100 / (1 + rs)) if rs > 0 else 0
                                
                                _, _, hist = macd_hist(df['close'].astype(float))
                                macd_hist_val = hist.iloc[-1] if not pd.isna(hist.iloc[-1]) else np.nan
                                
                                debug_data.append({
                                    'Ticker': ticker,
                                    'Date': latest_date.strftime("%Y-%m-%d"),
                                    'Close': f"{close:.2f}",
                                    'MA20': f"{ma20_val:.2f}" if not pd.isna(ma20_val) else "N/A",
                                    'MA50': f"{ma50_val:.2f}" if not pd.isna(ma50_val) else "N/A",
                                    'MA200': f"{ma200_val:.2f}" if not pd.isna(ma200_val) else "N/A",
                                    'RSI': f"{rsi_val:.2f}" if not np.isnan(rsi_val) else "N/A",
                                    'MACD_Hist': f"{macd_hist_val:.6f}" if not pd.isna(macd_hist_val) else "N/A",
                                    'MACD_Stage': breadth_df[breadth_df['ticker'] == ticker]['macd_stage'].iloc[0] if not breadth_df[breadth_df['ticker'] == ticker].empty else "N/A",
                                    'Bars': len(df)
                                })
                        except Exception as e:
                            st.caption(f"‚ö†Ô∏è {ticker}: Error - {str(e)[:50]}")
            
            if debug_data:
                debug_df = pd.DataFrame(debug_data)
                
                st.caption(f"**Showing first {len(debug_data)} tickers** (limited to 50 for performance)")
                st.caption(f"**Calculation method:** {'TA-Lib' if HAS_TALIB else 'Manual (Wilder\'s smoothing)'}")
                
                # Display table with scrolling
                st.dataframe(
                    debug_df,
                    height=400,
                    width='stretch',
                    column_config={
                        'Ticker': st.column_config.TextColumn('Ticker', width='small'),
                        'Date': st.column_config.TextColumn('Date', width='medium'),
                        'Close': st.column_config.TextColumn('Close', width='small'),
                        'MA20': st.column_config.TextColumn('MA20', width='small'),
                        'MA50': st.column_config.TextColumn('MA50', width='small'),
                        'MA200': st.column_config.TextColumn('MA200', width='small'),
                        'RSI': st.column_config.TextColumn('RSI', width='small'),
                        'MACD_Hist': st.column_config.TextColumn('MACD Hist', width='medium'),
                        'MACD_Stage': st.column_config.TextColumn('Stage', width='medium'),
                        'Bars': st.column_config.NumberColumn('Bars', width='small')
                    }
                )
                
                # Download debug table
                st.download_button(
                    "üì• Download Debug Table",
                    debug_df.to_csv(index=False).encode('utf-8'),
                    f"breadth_debug_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    "text/csv"
                )
                
                # Show summary statistics
                st.markdown("**Debug Summary:**")
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    avg_bars = debug_df['Bars'].astype(int).mean()
                    st.metric("Avg Bars/Ticker", f"{avg_bars:.0f}")
                with col2:
                    rsi_valid = len([x for x in debug_df['RSI'] if x != 'N/A'])
                    st.metric("Tickers with RSI", f"{rsi_valid}/{len(debug_df)}")
                with col3:
                    ma200_valid = len([x for x in debug_df['MA200'] if x != 'N/A'])
                    st.metric("Tickers with MA200", f"{ma200_valid}/{len(debug_df)}")
                with col4:
                    macd_valid = len([x for x in debug_df['MACD'] if x != 'N/A'])
                    st.metric("Tickers with MACD", f"{macd_valid}/{len(debug_df)}")
            else:
                if debug:  # Only show this warning when debug is enabled
                    st.warning("No debug data available for selected tickers")
        
        st.markdown("---")
        st.markdown("### üìä Historical Breadth Metrics")
        
        # Display historical metrics like current snapshot
        cols = st.columns(4)
        with cols[0]:
            st.metric("Total Tickers", latest['total_tickers'])
        with cols[1]:
            st.metric("Above MA20", f"{latest['ma20_above']}/{latest['total_tickers']}", f"{latest['ma20_pct']:.1f}%")
        with cols[2]:
            st.metric("Above MA50", f"{latest['ma50_above']}/{latest['total_tickers']}", f"{latest['ma50_pct']:.1f}%")
        with cols[3]:
            st.metric("Above MA200", f"{latest['ma200_above']}/{latest['total_tickers']}", f"{latest['ma200_pct']:.1f}%")
        
        # --- Moving Average Breadth ---
        st.markdown("#### üìà Moving Average Breadth")
        col1, col2, col3 = st.columns(3)
        
        ma20_above = latest['ma20_above']
        ma20_pct = latest['ma20_pct']
        
        ma50_above = latest['ma50_above']
        ma50_pct = latest['ma50_pct']
        
        ma200_above = latest['ma200_above']
        ma200_pct = latest['ma200_pct']
        
        with col1:
            st.metric("Above MA20", f"{ma20_pct:.1f}%", f"{ma20_above}/{latest['total_tickers']}")
            with st.expander("üìã View MA20 Tickers"):
                ma20_tickers = breadth_df[breadth_df['above_ma20']]['ticker'].tolist()
                if ma20_tickers:
                    st.markdown(f"**Tickers above MA20 ({len(ma20_tickers)}):**")
                    tickers_text = ", ".join(sorted(ma20_tickers))
                    st.text_area("", value=tickers_text, height=100, disabled=True, key="ma20_tickers")
                else:
                    st.info("No tickers above MA20")
        
        with col2:
            st.metric("Above MA50", f"{ma50_pct:.1f}%", f"{ma50_above}/{latest['total_tickers']}")
            with st.expander("üìã View MA50 Tickers"):
                ma50_tickers = breadth_df[breadth_df['above_ma50']]['ticker'].tolist()
                if ma50_tickers:
                    st.markdown(f"**Tickers above MA50 ({len(ma50_tickers)}):**")
                    tickers_text = ", ".join(sorted(ma50_tickers))
                    st.text_area("", value=tickers_text, height=100, disabled=True, key="ma50_tickers")
                else:
                    st.info("No tickers above MA50")
        
        with col3:
            st.metric("Above MA200", f"{ma200_pct:.1f}%", f"{ma200_above}/{latest['total_tickers']}")
            with st.expander("üìã View MA200 Tickers"):
                ma200_tickers = breadth_df[breadth_df['above_ma200']]['ticker'].tolist()
                if ma200_tickers:
                    st.markdown(f"**Tickers above MA200 ({len(ma200_tickers)}):**")
                    tickers_text = ", ".join(sorted(ma200_tickers))
                    st.text_area("", value=tickers_text, height=100, disabled=True, key="ma200_tickers")
                else:
                    st.info("No tickers above MA200")
        
        # MA Breadth chart
        ma_chart_data = pd.DataFrame({
            'Category': ['MA20', 'MA50', 'MA200'],
            'Percentage': [ma20_pct, ma50_pct, ma200_pct],
            'Count': [ma20_above, ma50_above, ma200_above],
            'Tickers': [
                ", ".join(sorted(ma20_tickers)),
                ", ".join(sorted(ma50_tickers)),
                ", ".join(sorted(ma200_tickers))
            ]
        })
        
        fig_ma = go.Figure()
        fig_ma.add_trace(go.Bar(
            x=ma_chart_data['Category'],
            y=ma_chart_data['Percentage'],
            marker_color=['#66bb6a' if x >= 50 else '#ff5252' for x in ma_chart_data['Percentage']],
            text=[f"{x:.1f}%" for x in ma_chart_data['Percentage']],
            textposition='auto',
            customdata=ma_chart_data[['Count', 'Tickers']],
            hovertemplate='<b>%{x}</b><br>%{y:.1f}%<br>Count: %{customdata[0]}<br><br><b>Tickers:</b><br>%{customdata[1]}<extra></extra>'
        ))
        fig_ma.update_layout(
            title="% of Stocks Above Moving Averages",
            yaxis_title="Percentage (%)",
            height=400,
            showlegend=False,
            hovermode='x'
        )
        fig_ma.add_hline(y=50, line_dash="dash", line_color="gray", annotation_text="50%")
        st.plotly_chart(fig_ma, width='stretch')
        
        # --- RSI Breadth ---
        st.markdown("#### üîÑ RSI Breadth")
        
        # Use historical data for RSI calculations, not current breadth_df
        hist_rsi_above_50 = latest['rsi_above_50']
        hist_rsi_below_50 = latest['rsi_below_50'] 
        hist_rsi_oversold = latest['rsi_oversold']
        hist_rsi_overbought = latest['rsi_overbought']
        hist_total_rsi = hist_rsi_above_50 + hist_rsi_below_50
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("RSI > 50", f"{(hist_rsi_above_50/hist_total_rsi*100):.1f}%" if hist_total_rsi > 0 else "0.0%", f"{hist_rsi_above_50}/{hist_total_rsi}")
            with st.expander("üìã View RSI > 50"):
                st.info("Historical snapshot - individual ticker lists not available. Use current snapshot for ticker details.")
        
        with col2:
            st.metric("RSI ‚â§ 50", f"{(hist_rsi_below_50/hist_total_rsi*100):.1f}%" if hist_total_rsi > 0 else "0.0%", f"{hist_rsi_below_50}/{hist_total_rsi}")
            with st.expander("üìã View RSI ‚â§ 50"):
                st.info("Historical snapshot - individual ticker lists not available. Use current snapshot for ticker details.")
        
        with col3:
            st.metric("Oversold (<30)", f"{(hist_rsi_oversold/hist_total_rsi*100):.1f}%" if hist_total_rsi > 0 else "0.0%", f"{hist_rsi_oversold}/{hist_total_rsi}")
            with st.expander("üìã View Oversold"):
                st.info("Historical snapshot - individual ticker lists not available. Use current snapshot for ticker details.")
        
        with col4:
            st.metric("Overbought (>70)", f"{(hist_rsi_overbought/hist_total_rsi*100):.1f}%" if hist_total_rsi > 0 else "0.0%", f"{hist_rsi_overbought}/{hist_total_rsi}")
            with st.expander("üìã View Overbought"):
                st.info("Historical snapshot - individual ticker lists not available. Use current snapshot for ticker details.")
        
        # Create a simplified RSI histogram showing only the aggregated data
        st.markdown("**Historical RSI Distribution:** Based on aggregated counts only")
        hist_rsi_data = pd.DataFrame({
            'RSI_Range': ['> 50', '‚â§ 50', 'Oversold (<30)', 'Overbought (>70)'],
            'Count': [hist_rsi_above_50, hist_rsi_below_50, hist_rsi_oversold, hist_rsi_overbought],
            'Percentage': [
                (hist_rsi_above_50/hist_total_rsi*100) if hist_total_rsi > 0 else 0,
                (hist_rsi_below_50/hist_total_rsi*100) if hist_total_rsi > 0 else 0,
                (hist_rsi_oversold/hist_total_rsi*100) if hist_total_rsi > 0 else 0,
                (hist_rsi_overbought/hist_total_rsi*100) if hist_total_rsi > 0 else 0
            ]
        })
        
        fig_rsi_hist = go.Figure()
        fig_rsi_hist.add_trace(go.Bar(
            x=hist_rsi_data['RSI_Range'],
            y=hist_rsi_data['Count'],
            text=[f"{p:.1f}%" for p in hist_rsi_data['Percentage']],
            textposition='auto',
            marker_color=['#4CAF50' if 'Oversold' in r else '#F44336' if 'Overbought' in r else '#2196F3' for r in hist_rsi_data['RSI_Range']]
        ))
        fig_rsi_hist.update_layout(
            title=f"RSI Distribution - {selected_date_str}",
            xaxis_title="RSI Range",
            yaxis_title="Number of Stocks",
            height=400,
            hovermode='x'
        )
        st.plotly_chart(fig_rsi_hist, width='stretch')
        
        # --- MACD Stage Breadth ---
        st.markdown("#### üìä MACD Histogram Stage Distribution")
        
        stage_counts = breadth_df['macd_stage'].value_counts()
        stage_order = [
            "1. Troughing",
            "2. Confirmed Trough",
            "3. Rising above Zero",
            "4. Peaking",
            "5. Confirmed Peak",
            "6. Falling below Zero"
        ]
        
        # Prepare data for all stages (including zero counts)
        stage_data = []
        for stage in stage_order:
            count = stage_counts.get(stage, 0)
            pct = (count / len(breadth_df) * 100) if len(breadth_df) > 0 else 0
            stage_tickers = breadth_df[breadth_df['macd_stage'] == stage]['ticker'].tolist()
            stage_data.append({
                'stage': stage,
                'count': count,
                'percentage': pct,
                'tickers': ", ".join(sorted(stage_tickers))
            })
        
        stage_summary_df = pd.DataFrame(stage_data)
        
        # Display metrics with expandable ticker lists
        cols = st.columns(6)
        for idx, row in stage_summary_df.iterrows():
            with cols[idx]:
                stage_name = row['stage'].split('. ')[1]
                st.metric(stage_name, f"{row['percentage']:.1f}%", f"{row['count']}/{len(breadth_df)}")
                with st.expander("üìã View"):
                    if row['tickers']:
                        st.markdown(f"**{row['stage']} ({row['count']}):**")
                        st.text_area("", value=row['tickers'], height=80, disabled=True, key=f"macd_tickers_{idx}")
                    else:
                        st.info("No tickers in this stage")
        
        # MACD stage chart with custom hover text
        stage_colors = {
            "1. Troughing": "#c8e6c9",
            "2. Confirmed Trough": "#39ff14",
            "3. Rising above Zero": "#2e7d32",
            "4. Peaking": "#ffccbc",
            "5. Confirmed Peak": "#ff5252",
            "6. Falling below Zero": "#c62828"
        }
        
        fig_macd = go.Figure()
        fig_macd.add_trace(go.Bar(
            x=stage_summary_df['stage'],
            y=stage_summary_df['percentage'],
            marker_color=[stage_colors.get(s, '#1f77b4') for s in stage_summary_df['stage']],
            text=[f"{p:.1f}%" for p in stage_summary_df['percentage']],
            textposition='auto',
            customdata=stage_summary_df[['count', 'tickers']],
            hovertemplate='<b>%{x}</b><br>%{y:.1f}%<br>Count: %{customdata[0]}<br><br><b>Tickers:</b><br>%{customdata[1]}<extra></extra>'
        ))
        fig_macd.update_layout(
            title="% of Stocks in Each MACD Stage",
            xaxis_title="MACD Stage",
            yaxis_title="Percentage (%)",
            height=400,
            showlegend=False,
            hovermode='x'
        )
        st.plotly_chart(fig_macd, width='stretch')
        
        # --- Market Sentiment Summary ---
        st.markdown("#### üí° Market Sentiment Summary")
        
        bullish_signals = 0
        bearish_signals = 0
        
        # MA breadth signals
        if ma20_pct > 60:
            bullish_signals += 1
        elif ma20_pct < 40:
            bearish_signals += 1
        
        if ma50_pct > 60:
            bullish_signals += 1
        elif ma50_pct < 40:
            bearish_signals += 1
        
        # RSI signals - FIXED: use hist_rsi variables instead of undefined rsi variables
        if hist_rsi_oversold > hist_rsi_overbought:
            bullish_signals += 1
        elif hist_rsi_overbought > hist_rsi_oversold:
            bearish_signals += 1
        
        # MACD signals - FIXED: stage_counts should come from historical data, not current breadth_df
        # Use current breadth_df stage_counts since we don't have historical stage breakdown
        bullish_macd = stage_counts.get("2. Confirmed Trough", 0) + stage_counts.get("1. Troughing", 0)
        bearish_macd = stage_counts.get("5. Confirmed Peak", 0) + stage_counts.get("4. Peaking", 0)
        
        if bullish_macd > bearish_macd:
            bullish_signals += 1
        elif bearish_macd > bullish_macd:
            bearish_signals += 1
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Bullish Signals", bullish_signals, help="Number of bullish market breadth indicators")
        with col2:
            st.metric("Bearish Signals", bearish_signals, help="Number of bearish market breadth indicators")
        with col3:
            if bullish_signals > bearish_signals:
                sentiment = "üü¢ Bullish"
                delta = f"+{bullish_signals - bearish_signals}"
            elif bearish_signals > bullish_signals:
                sentiment = "üî¥ Bearish"
                delta = f"-{bearish_signals - bullish_signals}"
            else:
                sentiment = "‚ö™ Neutral"
                delta = "0"
            st.metric("Overall Sentiment", sentiment, delta)
        
        # --- Download Data ---
        st.markdown("---")
        st.markdown("### üì• Download Data")
        
        # Prepare summary CSV - FIXED: use hist_rsi variables for historical snapshot
        summary_data = {
            'Metric': [
                'Above MA20 (%)', 'Above MA50 (%)', 'Above MA200 (%)',
                'RSI > 50 (%)', 'RSI <= 50 (%)', 'RSI Oversold (%)', 'RSI Overbought (%)'
            ],
            'Value': [
                f"{ma20_pct:.1f}",
                f"{ma50_pct:.1f}",
                f"{ma200_pct:.1f}",
                f"{(hist_rsi_above_50/hist_total_rsi*100):.1f}" if hist_total_rsi > 0 else "0.0",
                f"{(hist_rsi_below_50/hist_total_rsi*100):.1f}" if hist_total_rsi > 0 else "0.0",
                f"{(hist_rsi_oversold/hist_total_rsi*100):.1f}" if hist_total_rsi > 0 else "0.0",
                f"{(hist_rsi_overbought/hist_total_rsi*100):.1f}" if hist_total_rsi > 0 else "0.0"
            ],
            'Count': [
                f"{ma20_above}/{latest['total_tickers']}",
                f"{ma50_above}/{latest['total_tickers']}",
                f"{ma200_above}/{latest['total_tickers']}",
                f"{hist_rsi_above_50}/{hist_total_rsi}",
                f"{hist_rsi_below_50}/{hist_total_rsi}",
                f"{hist_rsi_oversold}/{hist_total_rsi}",
                f"{hist_rsi_overbought}/{hist_total_rsi}"
            ]
        }
        
        summary_df = pd.DataFrame(summary_data)
        
        col1, col2 = st.columns(2)
        with col1:
            st.download_button(
                "Download Summary CSV",
                summary_df.to_csv(index=False).encode('utf-8'),
                f"market_breadth_{datetime.now().strftime('%Y%m%d')}.csv",
                "text/csv"
            )
        
        with col2:
            st.download_button(
                "Download Detailed Data CSV",
                breadth_df.to_csv(index=False).encode('utf-8'),
                f"market_breadth_detailed_{datetime.now().strftime('%Y%m%d')}.csv",
                "text/csv"
            )
        
        # --- Historical Breadth Charts -----------------------------------------------
        if show_historical:
            st.markdown("---")
            st.markdown("### üìà Historical Market Breadth")
            
            # Check if recalculation requested
            if st.session_state.get('recalculate_breadth', False):
                with st.spinner(f"Recalculating {historical_days} days of historical breadth data..."):
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    # Calculate for each trading day going backwards from today
                    end_date_calc = datetime.now().date()
                    successful = 0
                    failed = 0
                    
                    # Use async batch processing for speed
                    import asyncio
                    
                    async def calculate_all_dates():
                        tasks = []
                        dates_to_calc = []
                        
                        for days_ago in range(historical_days):
                            target_date = end_date_calc - timedelta(days=days_ago)
                            
                            # Skip weekends (rough check)
                            if target_date.weekday() >= 5:
                                continue
                            
                            date_str = target_date.strftime("%Y-%m-%d")
                            dates_to_calc.append(date_str)
                            tasks.append(calculate_daily_breadth_async(date_str, all_tickers, lookback))
                        
                        # Run all calculations concurrently with progress tracking
                        results = []
                        for i, task in enumerate(asyncio.as_completed(tasks)):
                            result = await task
                            results.append(result)
                            progress_bar.progress((i + 1) / len(tasks))
                            status_text.text(f"Calculating breadth... ({i + 1}/{len(tasks)} completed)")
                        
                        return list(zip(dates_to_calc, results))
                    
                    # Run async calculations
                    try:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        date_results = loop.run_until_complete(calculate_all_dates())
                        loop.close()
                        
                        # Save results with improved error handling
                        for idx, (date_str, breadth_data) in enumerate(date_results):
                            status_text.text(f"Saving {date_str} ({idx + 1}/{len(date_results)})...")
                            
                            if isinstance(breadth_data, Exception):
                                failed += 1
                                error_msg = f"Error calculating {date_str}: {str(breadth_data)}"
                                if debug:
                                    st.error(error_msg)
                                    # Show more detailed error in expander
                                    with st.expander(f"üîç Detailed Error for {date_str}"):
                                        st.text(str(breadth_data))
                                else:
                                    st.caption(f"‚ö†Ô∏è Failed: {date_str} - {str(breadth_data)[:100]}...")
                            elif breadth_data and isinstance(breadth_data, dict):
                                try:
                                    # IMPROVED: Show debug info if available
                                    if debug and '_debug_info' in breadth_data:
                                        debug_info = breadth_data['_debug_info']
                                        if debug_info['error_count'] > 0 or debug_info['failed_tickers'] > 10:
                                            with st.expander(f"üîç Debug Info for {date_str}"):
                                                col1, col2, col3 = st.columns(3)
                                                with col1:
                                                    st.metric("Success Rate", f"{debug_info['successful_tickers']}/{debug_info['original_ticker_count']}")
                                                with col2:
                                                    st.metric("Error Count", debug_info['error_count'])
                                                with col3:
                                                    st.metric("RSI Valid", f"{debug_info['rsi_valid_count']}/{debug_info['successful_tickers']}")
                                                
                                                st.json(debug_info)
                                    
                                    # Remove debug info before saving
                                    breadth_data_clean = {k: v for k, v in breadth_data.items() if k != '_debug_info'}
                                    save_breadth_snapshot(date_str, breadth_data_clean, DB_PATH)
                                    successful += 1
                                except Exception as save_error:
                                    failed += 1
                                    error_msg = f"Error saving {date_str}: {str(save_error)}"
                                    if debug:
                                        st.error(error_msg)
                                    else:
                                        st.caption(f"‚ö†Ô∏è Save failed: {date_str} - {str(save_error)[:50]}...")
                            else:
                                failed += 1
                                if debug:
                                    st.warning(f"No data returned for {date_str}")
                            

                            progress_bar.progress((idx + 1) / len(date_results))
                        
                        status_text.empty()
                        progress_bar.empty()
                        st.success(f"‚úì Async calculation completed: {successful} successful, {failed} failed")
                        
                        # Show detailed error summary
                        if failed > 0:
                            st.warning(f"‚ö†Ô∏è {failed} days could not be calculated")
                            
                            if debug:
                                # Show error breakdown
                                error_types = {}
                                exception_count = 0
                                null_data_count = 0
                                save_error_count = 0
                                
                                for _, (date_str, breadth_data) in enumerate(date_results):
                                    if isinstance(breadth_data, Exception):
                                        exception_count += 1
                                        error_type = type(breadth_data).__name__
                                        error_types[error_type] = error_types.get(error_type, 0) + 1
                                    elif breadth_data is None:
                                        null_data_count += 1
                                
                                st.markdown("#### Error Breakdown")
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("Exceptions", exception_count)
                                with col2:
                                    st.metric("No Data", null_data_count)
                                with col3:
                                    st.metric("Save Errors", failed - exception_count - null_data_count)
                                
                                if error_types:
                                    st.markdown("**Exception Types:**")
                                    for error_type, count in error_types.items():
                                        st.caption(f"- {error_type}: {count} occurrences")
                    
                    except Exception as e:
                        st.error(f"‚ùå Async calculation error: {str(e)}")
                        
                        # Show full traceback in debug mode
                        if debug:
                            import traceback
                            st.markdown("#### Full Error Traceback")
                            st.code(traceback.format_exc())
                        
                        # Fallback to synchronous if async fails
                        st.info("Falling back to synchronous calculation...")
                        
                        for days_ago in range(historical_days):
                            target_date = end_date_calc - timedelta(days=days_ago)
                            date_str = target_date.strftime("%Y-%m-%d")
                            
                            if target_date.weekday() >= 5:
                                continue
                            
                            status_text.text(f"Processing {date_str} ({days_ago + 1}/{historical_days})...")
                            
                            try:
                                breadth_data = calculate_daily_breadth(date_str, all_tickers, lookback, DB_PATH)
                                if breadth_data:
                                    save_breadth_snapshot(date_str, breadth_data, DB_PATH)
                                    successful += 1
                                else:
                                    failed += 1
                                    if debug:
                                        st.caption(f"‚ö†Ô∏è No data for {date_str}")
                            except Exception as sync_error:
                                failed += 1
                                if debug:
                                    st.error(f"Sync error calculating {date_str}: {str(sync_error)}")
                                else:
                                    st.caption(f"‚ö†Ô∏è Sync failed: {date_str}")
                            

                            progress_bar.progress((days_ago + 1) / historical_days)
                        
                        status_text.empty()
                        progress_bar.empty()
                        st.success(f"‚úì Sync calculation completed: {successful} successful, {failed} failed")
            
            st.session_state['recalculate_breadth'] = False
            st.rerun()
        
        # Load historical data
        hist_df = load_breadth_history(days=historical_days, db_path=DB_PATH)
        
        if hist_df.empty:
            st.warning("üì≠ No historical data available. Click 'üîÑ Recalculate Historical Data' button in the sidebar to generate it.")
            st.info(f"""
            **Note:** This will calculate breadth metrics for the past {historical_days} days using async processing.
            - Each day requires data from 300 days prior for MA200 calculation
            - Async batch processing significantly reduces calculation time
            - Weekend days will be skipped automatically
            - Estimated time: ~{historical_days * 0.1:.0f} seconds ({historical_days * 0.1 / 60:.1f} minutes) with async
            """)
        else:
            st.success(f"‚úÖ Loaded {len(hist_df)} days of historical data (from {hist_df['date'].min().date()} to {hist_df['date'].max().date()})")
            
            # Calculate bullish/bearish percentages FIRST (before merging)
            hist_df['macd_bullish_pct'] = ((hist_df['macd_troughing'].astype(int) + hist_df['macd_confirmed_trough'].astype(int) + hist_df['macd_rising'].astype(int)) / hist_df['total_tickers'].astype(int) * 100).fillna(0)
            hist_df['macd_bearish_pct'] = ((hist_df['macd_peaking'].astype(int) + hist_df['macd_confirmed_peak'].astype(int) + hist_df['macd_falling'].astype(int)) / hist_df['total_tickers'].astype(int) * 100).fillna(0)
            
            # --- VNINDEX Technical Analysis with Market Breadth Context ---
            st.markdown("---")
            st.markdown("### üìä VNINDEX Technical Analysis with Market Breadth Context")
            
            st.info("üí° **Tip:** Hover over any date on the charts below to see a vertical line across all subplots with synchronized data.")
            
            # Calculate bullish/bearish percentages FIRST (before merging)
            hist_df['macd_bullish_pct'] = ((hist_df['macd_troughing'].astype(int) + hist_df['macd_confirmed_trough'].astype(int) + hist_df['macd_rising'].astype(int)) / hist_df['total_tickers'].astype(int) * 100).fillna(0)
            hist_df['macd_bearish_pct'] = ((hist_df['macd_peaking'].astype(int) + hist_df['macd_confirmed_peak'].astype(int) + hist_df['macd_falling'].astype(int)) / hist_df['total_tickers'].astype(int) * 100).fillna(0)
            
            # Load VNINDEX data - USE SAME DATE RANGE AS HISTORICAL BREADTH DATA
            vnindex_start = hist_df['date'].min().date() - timedelta(days=100)  # Extra days for EMA50 calculation
            vnindex_end = hist_df['date'].max().date()
            vnindex_df = load_price_range('VNINDEX', vnindex_start, vnindex_end)
            
            if vnindex_df.empty:
                st.warning("‚ö†Ô∏è VNINDEX data not available.")
            else:
                # Sort by date ASCENDING for proper TA calculations
                vnindex_df = vnindex_df.sort_values('date', ascending=True).copy()
                
                # Calculate technical indicators
                vnindex_df['close'] = pd.to_numeric(vnindex_df['close'], errors='coerce')
                
                close_array = vnindex_df['close'].values.astype(np.float64)
                
                # Use TA-Lib if available, otherwise manual
                if HAS_TALIB:
                    try:
                        vnindex_df['ema10'] = talib.EMA(close_array, timeperiod=10)
                        vnindex_df['ema20'] = talib.EMA(close_array, timeperiod=20)
                        vnindex_df['ema50'] = talib.EMA(close_array, timeperiod=50)
                        vnindex_df['rsi'] = talib.RSI(close_array, timeperiod=14)
                        
                        # Bollinger Bands
                        bb_upper, bb_middle, bb_lower = talib.BBANDS(close_array, timeperiod=20, nbdevup=2, nbdevdn=2)
                        vnindex_df['bb_upper'] = bb_upper
                        vnindex_df['bb_middle'] = bb_middle
                        vnindex_df['bb_lower'] = bb_lower
                        
                        # MACD
                        macd_line, macd_signal, macd_hist_vals = talib.MACD(close_array, fastperiod=12, slowperiod=26, signalperiod=9)
                        vnindex_df['macd_line'] = macd_line
                        vnindex_df['macd_signal'] = macd_signal
                        vnindex_df['macd_hist'] = macd_hist_vals
                    except Exception as e:
                        st.warning(f"‚ö†Ô∏è TA-Lib calculation error: {e}. Using manual calculations.")
                        # Fallback to manual
                        vnindex_df['ema10'] = vnindex_df['close'].ewm(span=10, adjust=False).mean()
                        vnindex_df['ema20'] = vnindex_df['close'].ewm(span=20, adjust=False).mean()
                        vnindex_df['ema50'] = vnindex_df['close'].ewm(span=50, adjust=False).mean()
                        vnindex_df['rsi'] = calculate_rsi_manual(vnindex_df)
                        vnindex_df['bb_middle'] = vnindex_df['close'].rolling(20).mean()
                        bb_std = vnindex_df['close'].rolling(20).std()
                        vnindex_df['bb_upper'] = vnindex_df['bb_middle'] + (bb_std * 2)
                        vnindex_df['bb_lower'] = vnindex_df['bb_middle'] - (bb_std * 2)
                        macd_line, macd_signal, macd_hist_vals = macd_hist(vnindex_df['close'])
                        vnindex_df['macd_line'] = macd_line
                        vnindex_df['macd_signal'] = macd_signal
                        vnindex_df['macd_hist'] = macd_hist_vals
                else:
                    # Manual calculation
                    vnindex_df['ema10'] = vnindex_df['close'].ewm(span=10, adjust=False).mean()
                    vnindex_df['ema20'] = vnindex_df['close'].ewm(span=20, adjust=False).mean()
                    vnindex_df['ema50'] = vnindex_df['close'].ewm(span=50, adjust=False).mean()
                    vnindex_df['rsi'] = calculate_rsi_manual(vnindex_df)
                    vnindex_df['bb_middle'] = vnindex_df['close'].rolling(20).mean()
                    bb_std = vnindex_df['close'].rolling(20).std()
                    vnindex_df['bb_upper'] = vnindex_df['bb_middle'] + (bb_std * 2)
                    vnindex_df['bb_lower'] = vnindex_df['bb_middle'] - (bb_std * 2)
                    macd_line, macd_signal, macd_hist_vals = macd_hist(vnindex_df['close'])
                    vnindex_df['macd_line'] = macd_line
                    vnindex_df['macd_signal'] = macd_signal
                    vnindex_df['macd_hist'] = macd_hist_vals
                
                # Merge with historical breadth - maintain ascending order
                vnindex_merged = vnindex_df.merge(
                    hist_df[['date', 'ma20_pct', 'rsi_oversold', 'rsi_overbought', 'total_tickers', 'macd_bullish_pct', 'macd_bearish_pct']],
                    on='date',
                    how='left'
                )
                
                # Filter to breadth date range and maintain ascending order
                vnindex_merged = vnindex_merged[
                    (vnindex_merged['date'] >= hist_df['date'].min()) &
                    (vnindex_merged['date'] <= hist_df['date'].max())                ].sort_values('date', ascending=True).reset_index(drop=True)
                
                # Define peak/bottom regions based on breadth indicators
                # Bottom region: MA20 < 30% OR RSI oversold > 20% OR MACD bullish > 60%
                vnindex_merged['is_bottom'] = (
                    (vnindex_merged['ma20_pct'] < 30) |
                    ((vnindex_merged['rsi_oversold'] / vnindex_merged['total_tickers'] * 100) > 20) |
                    (vnindex_merged['macd_bullish_pct'] > 60)
                )
                
                # Peak region: MA20 > 70% OR RSI overbought > 20% OR MACD bearish > 60%
                vnindex_merged['is_peak'] = (
                    (vnindex_merged['ma20_pct'] > 70) |
                    ((vnindex_merged['rsi_overbought'] / vnindex_merged['total_tickers'] * 100) > 20) |
                    (vnindex_merged['macd_bearish_pct'] > 60)
                )
                
                # Create figure with subplots
                fig_vnindex = make_subplots(
                    rows=4, cols=1,
                    row_heights=[0.4, 0.2, 0.2, 0.2],
                    subplot_titles=('VNINDEX Price with EMAs & Bollinger Bands', 'Volume', 'RSI (14)', 'MACD Histogram'),
                    vertical_spacing=0.05,
                    specs=[[{"secondary_y": False}], [{"secondary_y": False}], [{"secondary_y": False}], [{"secondary_y": False}]]
                )
                
                # Add shaded regions for peaks and bottoms to all subplots
                for row in range(1, 5):
                    # Add bottom regions (green shade) - find contiguous regions
                    bottom_dates = vnindex_merged[vnindex_merged['is_bottom']]['date']
                    if len(bottom_dates) > 0:
                        # Group contiguous dates
                        regions = []
                        start_date = bottom_dates.iloc[0]
                        prev_idx = vnindex_merged[vnindex_merged['date'] == start_date].index[0]
                        
                        for i, date in enumerate(bottom_dates):
                            curr_idx = vnindex_merged[vnindex_merged['date'] == date].index[0]
                            # Check if this is a new region (gap in indices)
                            if i > 0 and curr_idx - prev_idx > 1:
                                # End previous region
                                regions.append((start_date, bottom_dates.iloc[i-1]))
                                start_date = date
                            prev_idx = curr_idx
                        
                        # Add last region
                        regions.append((start_date, bottom_dates.iloc[-1]))
                        
                        # Draw rectangles for each region
                        for start, end in regions:
                            fig_vnindex.add_vrect(
                                x0=start, x1=end,
                                fillcolor="green", opacity=0.1, layer="below", line_width=0,
                                row=row, col=1
                            )
                    
                    # Add peak regions (red shade) - find contiguous regions
                    peak_dates = vnindex_merged[vnindex_merged['is_peak']]['date']
                    if len(peak_dates) > 0:
                        # Group contiguous dates
                        regions = []
                        start_date = peak_dates.iloc[0]
                        prev_idx = vnindex_merged[vnindex_merged['date'] == start_date].index[0]
                        
                        for i, date in enumerate(peak_dates):
                            curr_idx = vnindex_merged[vnindex_merged['date'] == date].index[0]
                            # Check if this is a new region (gap in indices)
                            if i > 0 and curr_idx - prev_idx > 1:
                                # End previous region
                                regions.append((start_date, peak_dates.iloc[i-1]))
                                start_date = date
                            prev_idx = curr_idx
                        
                        # Add last region
                        regions.append((start_date, peak_dates.iloc[-1]))
                        
                        # Draw rectangles for each region
                        for start, end in regions:
                            fig_vnindex.add_vrect(
                                x0=start, x1=end,
                                fillcolor="red", opacity=0.1, layer="below", line_width=0,
                                row=row, col=1
                            )
                
                # Row 1: Price with EMAs and Bollinger Bands
                fig_vnindex.add_trace(go.Scatter(
                    x=vnindex_merged['date'], y=vnindex_merged['close'],
                    name='VNINDEX', line=dict(color='#1f77b4', width=2)
                ), row=1, col=1)
                
                fig_vnindex.add_trace(go.Scatter(
                    x=vnindex_merged['date'], y=vnindex_merged['ema10'],
                    name='EMA10', line=dict(color='#ff7f0e', width=1, dash='dot')
                ), row=1, col=1)
                
                fig_vnindex.add_trace(go.Scatter(
                    x=vnindex_merged['date'], y=vnindex_merged['ema20'],
                    name='EMA20', line=dict(color='#2ca02c', width=1, dash='dash')
                ), row=1, col=1)
                
                fig_vnindex.add_trace(go.Scatter(
                    x=vnindex_merged['date'], y=vnindex_merged['ema50'],
                    name='EMA50', line=dict(color='#d62728', width=1.5)
                ), row=1, col=1)
                
                # Bollinger Bands
                fig_vnindex.add_trace(go.Scatter(
                    x=vnindex_merged['date'], y=vnindex_merged['bb_upper'],
                    name='BB Upper', line=dict(color='gray', width=1, dash='dot'),
                    showlegend=False
                ), row=1, col=1)
                
                fig_vnindex.add_trace(go.Scatter(
                    x=vnindex_merged['date'], y=vnindex_merged['bb_lower'],
                    name='BB Lower', line=dict(color='gray', width=1, dash='dot'),
                    fill='tonexty', fillcolor='rgba(128,128,128,0.1)',
                    showlegend=False
                ), row=1, col=1)
                
                # Row 2: Volume
                fig_vnindex.add_trace(go.Bar(
                    x=vnindex_merged['date'], y=vnindex_merged['volume'],
                    name='Volume', marker_color='#9467bd', showlegend=False
                ), row=2, col=1)
                
                # Row 3: RSI
                fig_vnindex.add_trace(go.Scatter(
                    x=vnindex_merged['date'], y=vnindex_merged['rsi'],
                    name='RSI', line=dict(color='#8c564b', width=2),
                    showlegend=False
                ), row=3, col=1)
                
                fig_vnindex.add_hline(y=70, line_dash="dash", line_color="red", row=3, col=1, annotation_text="Overbought (70)")
                fig_vnindex.add_hline(y=30, line_dash="dash", line_color="green", row=3, col=1, annotation_text="Oversold (30)")
               
                fig_vnindex.add_hline(y=50, line_dash="dot", line_color="gray", row=3, col=1)
                
                # Row 4: MACD Histogram
                colors = ['#2ca02c' if v >= 0 else '#d62728' for v in vnindex_merged['macd_hist']]
                fig_vnindex.add_trace(go.Bar(
                    x=vnindex_merged['date'], y=vnindex_merged['macd_hist'],
                    name='MACD Hist', marker_color=colors,
                    showlegend=False
                ), row=4, col=1)
                
                fig_vnindex.add_trace(go.Scatter(
                    x=vnindex_merged['date'], y=vnindex_merged['macd_line'],
                    name='MACD Line', line=dict(color='blue', width=1.5),
                    showlegend=False
                ), row=4, col=1)
                
                fig_vnindex.add_trace(go.Scatter(
                    x=vnindex_merged['date'], y=vnindex_merged['macd_signal'],
                    name='Signal', line=dict(color='orange', width=1.5),
                    showlegend=False
                ), row=4, col=1)
                
                fig_vnindex.add_hline(y=0, line_dash="solid", line_color="black", row=4, col=1)
                
                # Update layout with synchronized hover
                fig_vnindex.update_layout(
                    height=1000,
                    hovermode='x unified',
                    showlegend=True,
                    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
                    title_text="VNINDEX Technical Analysis with Market Breadth Context",
                    xaxis=dict(matches='x'),
                    xaxis2=dict(matches='x'),
                    xaxis3=dict(matches='x'),
                    xaxis4=dict(matches='x')
                )
                
                # Add prominent vertical line on hover for all x-axes
                for i in range(1, 5):
                    fig_vnindex.update_xaxes(
                        showspikes=True,
                        spikemode='across',
                        spikesnap='cursor',
                        spikecolor='rgba(255,0,0,0.5)',  # More visible red color
                        spikethickness=2,  # Thicker line
                        spikedash='solid',
                        row=i, col=1
                    )
                
                st.plotly_chart(fig_vnindex, width='stretch')
                
                # Add interpretation guide
                with st.expander("üìñ How to Read This Chart"):
                    st.markdown("""
                    **Shaded Regions:**
                    - üü¢ **Green shading**: Market bottom conditions detected
                      - MA20 breadth < 30% OR
                      - High RSI oversold (>20% of stocks) OR
                      - High MACD bullish (>60% bullish stages)
                    
                    - üî¥ **Red shading**: Market peak conditions detected
                      - MA20 breadth > 70% OR
                      - High RSI overbought (>20% of stocks) OR
                      - High MACD bearish (>60% bearish stages)
                    
                    **Technical Indicators:**
                    - **EMAs (10/20/50)**: Trend direction and support/resistance levels
                    - **Bollinger Bands**: Volatility and potential reversal zones
                    - **RSI**: Overbought (>70) / Oversold (<30) momentum
                    - **MACD Histogram**: Trend strength and momentum shifts
                    
                    **Trading Signals:**
                    - Look for bullish setups in green (bottom) regions
                    - Consider taking profits or caution in red (peak) regions
                    - Confirm signals across multiple timeframes and breadth indicators
                    """)
            
            # --- Moving Average Breadth History ---
            st.markdown("#### üìä Moving Average Breadth (Historical)")
            
            fig_ma_hist = go.Figure()
            fig_ma_hist.add_trace(go.Scatter(
                x=hist_df['date'], y=hist_df['ma20_pct'],
                name='MA20', line=dict(color='#2196F3', width=2)
            ))
            fig_ma_hist.add_trace(go.Scatter(
                x=hist_df['date'], y=hist_df['ma50_pct'],
                name='MA50', line=dict(color='#FF9800', width=2)
            ))
            fig_ma_hist.add_trace(go.Scatter(
                x=hist_df['date'], y=hist_df['ma200_pct'],
                name='MA200', line=dict(color='#9C27B0', width=2)
            ))
            
            fig_ma_hist.add_hline(y=50, line_dash="dash", line_color="gray", annotation_text="50%")
            fig_ma_hist.add_hline(y=70, line_dash="dot", line_color="green", annotation_text="Bullish (70%)")
            fig_ma_hist.add_hline(y=30, line_dash="dot", line_color="red", annotation_text="Bearish (30%)")
            
            fig_ma_hist.update_layout(
                title="% of Stocks Above Moving Averages (Time Series)",
                xaxis_title="Date",
                yaxis_title="Percentage (%)",
                height=500,
                hovermode='x unified',
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
            )
            
            # Add prominent vertical line on hover
            fig_ma_hist.update_xaxes(
                showspikes=True,
                spikemode='across',
                spikesnap='cursor',
                spikecolor='rgba(255,0,0,0.5)',  # More visible red
                spikethickness=2,  # Thicker
                spikedash='solid'
            )
            
            st.plotly_chart(fig_ma_hist, width='stretch')
            
            # --- RSI Breadth History ---
            st.markdown("#### üîÑ RSI Breadth (Historical)")
            
            fig_rsi_hist = make_subplots(
                rows=2, cols=1, 
                subplot_titles=("RSI Above/Below 50", "RSI Oversold/Overbought"),
                vertical_spacing=0.15,
                row_heights=[0.5, 0.5]
           
            )
            
            # RSI above/below 50 - ensure we're working with numeric Series
            total_rsi_series = hist_df['rsi_above_50'].astype(int) + hist_df['rsi_below_50'].astype(int)
            # Avoid division by zero
            total_rsi_series = total_rsi_series.replace(0, 1)
            
            rsi_above_50_pct = (hist_df['rsi_above_50'].astype(int) / total_rsi_series * 100).fillna(0)
            rsi_below_50_pct = (hist_df['rsi_below_50'].astype(int) / total_rsi_series * 100).fillna(0)
            
            fig_rsi_hist.add_trace(go.Scatter(
                x=hist_df['date'], y=rsi_above_50_pct,
                name='RSI > 50', fill='tozeroy', line=dict(color='#4CAF50')
            ), row=1, col=1)
            fig_rsi_hist.add_trace(go.Scatter(
                x=hist_df['date'], y=rsi_below_50_pct,
                name='RSI ‚â§ 50', fill='tozeroy', line=dict(color='#F44336')
            ), row=1, col=1)
            
            # RSI oversold/overbought
            rsi_oversold_pct = (hist_df['rsi_oversold'].astype(int) / total_rsi_series * 100).fillna(0)
            rsi_overbought_pct = (hist_df['rsi_overbought'].astype(int) / total_rsi_series * 100).fillna(0)
            
            fig_rsi_hist.add_trace(go.Scatter(
                x=hist_df['date'], y=rsi_oversold_pct,
                name='Oversold (<30)', line=dict(color='#00BCD4', width=2)
            ), row=2, col=1)
            fig_rsi_hist.add_trace(go.Scatter(
                x=hist_df['date'], y=rsi_overbought_pct,
                name='Overbought (>70)', line=dict(color='#FF5722', width=2)
            ), row=2, col=1)
            
            fig_rsi_hist.update_xaxes(title_text="Date", row=2, col=1)
            fig_rsi_hist.update_yaxes(title_text="Percentage (%)", row=1, col=1)
            fig_rsi_hist.update_yaxes(title_text="Percentage (%)", row=2, col=1)
            
            fig_rsi_hist.update_layout(
                height=700,
                hovermode='x unified',
                showlegend=True,
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
                xaxis=dict(matches='x'),
                xaxis2=dict(matches='x')
            )
            
            # Add prominent vertical line on hover for both subplots
            for row_num in [1, 2]:
                fig_rsi_hist.update_xaxes(
                    showspikes=True,
                    spikemode='across',
                    spikesnap='cursor',
                    spikecolor='rgba(255,0,0,0.5)',  # More visible red
                    spikethickness=2,  # Thicker
                    spikedash='solid',
                    row=row_num, col=1
                )
            
            st.plotly_chart(fig_rsi_hist, width='stretch')
            
            # --- MACD Stage Distribution History ---
            st.markdown("#### üìä MACD Stage Distribution (Historical)")
            
            fig_macd_hist = make_subplots(
                rows=2, cols=1,
                line=dict(color='#c8e6c9', width=1.5),
                subplot_titles=("Bullish vs Bearish MACD Stages", "Detailed MACD Stage Breakdown"),
                vertical_spacing=0.15,
                row_heights=[0.4, 0.6],
            )
            
            # Bullish vs Bearish
            fig_macd_hist.add_trace(go.Scatter(
                x=hist_df['date'], y=hist_df['macd_bullish_pct'],
                name='Bullish Stages', fill='tozeroy', line=dict(color='#4CAF50', width=2)
            ), row=1, col=1)

            fig_macd_hist.add_trace(go.Scatter(
                x=hist_df['date'], y=hist_df['macd_bearish_pct'],  
                name='Bearish Stages', fill='tozeroy', line=dict(color='#F44336', width=2)),
                row=2, col=1)
            
            # Detailed breakdown line=dict(color='#ff5252', width=2)
            fig_macd_hist.add_trace(go.Scatter(
                x=hist_df['date'], y=hist_df['macd_troughing'],
                name='Troughing', line=dict(color='#c8e6c9', width=2)
            ), row=2, col=1)
            
            fig_macd_hist.add_trace(go.Scatter(
                x=hist_df['date'], y=hist_df['macd_confirmed_trough'],
                name='Confirmed Trough', line=dict(color='#39ff14', width=2)
            ), row=2, col=1)

            fig_macd_hist.add_trace(go.Scatter(
                x=hist_df['date'], y=hist_df['macd_rising'],
                name='Rising', line=dict(color='#2e7d32', width=2)
            ), row=2, col=1)

            fig_macd_hist.add_trace(go.Scatter(
                x=hist_df['date'], y=hist_df['macd_peaking'],
                name='Peaking', line=dict(color='#ffccbc', width=2)
            ), row=2, col=1)

            fig_macd_hist.add_trace(go.Scatter(
                x=hist_df['date'], y=hist_df['macd_confirmed_peak'],
                name='Confirmed Peak', line=dict(color='#ff5252', width=2)
            ), row=2, col=1)          
            
              # Add prominent vertical line on hover for both subplots
            st.markdown("### üìä End of Historical Analysis")   
  
            
            st.plotly_chart(fig_macd_hist, width='stretch')
            
            # --- Market Sentiment Summary (Historical) ---
            st.markdown("#### üí° Market Sentiment Summary (Historical)")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Bullish Signals", latest_sentiment['macd_bullish_pct'], help="Percentage of bullish MACD stages")
            with col2:
                st.metric("Bearish Signals", latest_sentiment['macd_bearish_pct'], help="Percentage of bearish MACD stages")
            with col3:
                if latest_sentiment['macd_bullish_pct'] > latest_sentiment['macd_bearish_pct']:
                    delta = f"+{latest_sentiment['macd_bullish_pct'] - latest_sentiment['macd_bearish_pct']}"
                    sentiment = "üü¢ Bullish"
                elif latest_sentiment['macd_bearish_pct'] > latest_sentiment['macd_bullish_pct']:
                    delta = f"-{latest_sentiment['macd_bearish_pct'] - latest_sentiment['macd_bullish_pct']}"
                    sentiment = "üî¥ Bearish"
                else:
                    delta = "0"
                    sentiment = "‚ö™ Neutral"
                st.metric("Overall Sentiment", sentiment, delta)
            
            # --- Download Historical Data ---
            st.markdown("---")
            st.markdown("### üì• Download Historical Data")
            
            # Prepare summary CSV - FIXED: use hist_rsi variables for historical snapshot
            hist_summary_data = {
                'Date': hist_df['date'].dt.strftime("%Y-%m-%d"),
                'Above MA20 (%)': (hist_df['ma20_above'] / hist_df['total_tickers'] * 100).fillna(0),
                'Above MA50 (%)': (hist_df['ma50_above'] / hist_df['total_tickers'] * 100).fillna(0),
                'Above MA200 (%)': (hist_df['ma200_above'] / hist_df['total_tickers'] * 100).fillna(0),
                'RSI > 50 (%)': (hist_df['rsi_above_50'] / hist_df['total_tickers'] * 100).fillna(0),
                'RSI ‚â§ 50 (%)': (hist_df['rsi_below_50'] / hist_df['total_tickers'] * 100).fillna(0),
                'RSI Oversold (%)': (hist_df['rsi_oversold'] / hist_df['total_tickers'] * 100).fillna(0),
                'RSI Overbought (%)': (hist_df['rsi_overbought'] / hist_df['total_tickers'] * 100).fillna(0),
                'MACD Bullish (%)': hist_df['macd_bullish_pct'],
                'MACD Bearish (%)': hist_df['macd_bearish_pct'],
                'Total Tickers': hist_df['total_tickers']
            }
            
            hist_summary_df = pd.DataFrame(hist_summary_data)
            
            col1, col2 = st.columns(2)
            with col1:
                st.download_button(
                    "Download Summary CSV",
                    hist_summary_df.to_csv(index=False).encode('utf-8'),
                    f"market_breadth_historical_{datetime.now().strftime('%Y%m%d')}.csv",
                    "text/csv"
                )
            
            with col2:
                st.download_button(
                    "Download Detailed Data CSV",
                    hist_df.to_csv(index=False).encode('utf-8'),
                    f"market_breadth_historical_detailed_{datetime.now().strftime('%Y%m%d')}.csv",
                    "text/csv"
                )
        
        # --- End of Historical Analysis Section ---
        st.markdown("---")
        st.markdown("### üìä End of Historical Analysis")
